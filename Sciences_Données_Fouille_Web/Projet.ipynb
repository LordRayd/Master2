{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "af0213e7f25141ee04fe389dc8ad6b35b026bad001b81753d76493b1b6cea4f1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Projet flux Rss\n",
    "## TP 1 - Feed Collector\n",
    "### Import"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import shelve\n",
    "import time\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from subprocess import check_output\n",
    "from datetime import datetime\n",
    "import fileinput\n",
    "\n",
    "import feedparser\n",
    "import langdetect\n",
    "import chardet\n",
    "from bs4 import BeautifulSoup\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "import lxml\n",
    "from lxml.html.clean import Cleaner\n",
    "import io"
   ]
  },
  {
   "source": [
    "### Item Rss"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Item_RSS:\n",
    "    \"\"\"\n",
    "    Représente un item Rss obtenu depuis le flux \n",
    "\n",
    "    source_feed : L url de la source du flux\n",
    "    local_url : L url du fichier local contenant la page de l'item Rss\n",
    "    lang : La langue utilisé dans le texte de l'item Rss\n",
    "    date : La date de l'item Rss\n",
    "    target_data : Le contenu de la page source de l'item Rss\n",
    "    \"\"\"\n",
    "    id = None\n",
    "    title = None\n",
    "    summary = None\n",
    "    description = None\n",
    "    all_links = None\n",
    "    source_post = None\n",
    "    source_feed = None\n",
    "    lang = None\n",
    "    date = None\n",
    "    target_data = None\n",
    "    type_flux = None\n",
    "\n",
    "    def __init__(self, post, feed, tool=None, type_flux=['default']):\n",
    "        \"\"\"\n",
    "        Initialise l item rss a partir des données récupérés depuis le flux\n",
    "\n",
    "        Paramètres:\n",
    "            post : L'item Rss recupéré depuis le flux\n",
    "            feed : Les elements decrivants le flux\n",
    "        \"\"\"\n",
    "        self.tool = tool\n",
    "        self.type_flux = type_flux\n",
    "        self.type_predit = 'default'\n",
    "        if  hasattr(post, 'title'):\n",
    "            self.tile = post.title\n",
    "            self.lang = langdetect.detect(post.title)\n",
    "        if  hasattr(post, 'summary'):\n",
    "            self.summary = post.summary\n",
    "        if  hasattr(post, 'description'):\n",
    "            self.description = post.description\n",
    "        if hasattr(post, 'links'):\n",
    "            self.all_links = post.links\n",
    "        if hasattr(feed, 'link'):\n",
    "            self.source_feed = feed.link\n",
    "        self.integrity_construct()\n",
    "        if  hasattr(post, 'link'):\n",
    "            self.source_post = post.link\n",
    "            self.id = hashlib.sha224(post.link.encode(encoding='UTF-8')).hexdigest()\n",
    "            self.set_target_data_from_url(post.link)\n",
    "        self.date = datetime.now()\n",
    "\n",
    "    def set_target_data_from_url(self, url):\n",
    "        cleaner = Cleaner()\n",
    "        cleaner.javascript = True # Enleve les balises script et leur contenu\n",
    "        cleaner.style = True      # Enleve les balises style et leur contenu\n",
    "        \n",
    "        try:\n",
    "            html = urllib.request.urlopen(url)\n",
    "            soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "            # ====== PLUS DE BALISE SCRIPT & STYLE ==========\n",
    "            target_data = lxml.html.tostring(cleaner.clean_html(lxml.html.document_fromstring(str(soup.prettify())))) \n",
    "\n",
    "            s = BeautifulSoup(target_data)\n",
    "            self.target_data = os.linesep.join([s.lstrip() for s in s.getText().splitlines() if s and s.strip() != ''])\n",
    "        except urllib.error.HTTPError as e:\n",
    "            self.target_data = None\n",
    "        except urllib.error.URLError as e:\n",
    "            self.target_data = None\n",
    "\n",
    "    def affichage(self):\n",
    "        \"\"\"\n",
    "        Affiche tous les éléments de l'item Rss si ils ne sont pas vides\n",
    "        \"\"\"\n",
    "        self.print_id()\n",
    "        self.print_title()\n",
    "        self.print_summary()\n",
    "        self.print_description()\n",
    "        self.print_source_post()\n",
    "        self.print_source_feed()\n",
    "        self.print_lang()\n",
    "        self.print_date()\n",
    "        self.print_target_data()\n",
    "        self.print_type_flux()\n",
    "\n",
    "    def print_id(self):\n",
    "        if self.id != None:\n",
    "            print('id : ', self.id, '\\n')\n",
    "    def print_title(self) :\n",
    "        if self.title != None:\n",
    "            print('title : ', self.title, '\\n')\n",
    "    def print_summary(self) : \n",
    "        if self.summary != None:\n",
    "            print('summary : ', self.summary, '\\n')\n",
    "    def print_description(self) : \n",
    "        if self.description != None:\n",
    "            print('description : ', self.description, '\\n')\n",
    "    def print_source_post(self) : \n",
    "        if self.source_post != None:\n",
    "            print('source_post : ', self.source_post, '\\n')\n",
    "    def print_source_feed(self) :\n",
    "        if self.source_feed != None:\n",
    "            print('source_feed : ', self.source_feed, '\\n')\n",
    "    def print_lang(self) : \n",
    "        if self.lang != None:\n",
    "            print('lang : ', self.lang, '\\n')\n",
    "    def print_date(self) :\n",
    "        if self.date != None:\n",
    "            print('date : ', self.date, '\\n')\n",
    "    def print_target_data(self) : \n",
    "        if self.target_data != None:\n",
    "            print('target_data : ', self.target_data, '\\n')\n",
    "    def print_type_flux(self):\n",
    "        if self.type_flux != None :\n",
    "            print('type : ', self.type_flux, '\\n')\n",
    "    def print_type_predit(self):\n",
    "        if self.type_predit != None :\n",
    "            print('type predit : ', self.type_predit, '\\n')\n",
    "\n",
    "    def integrity_construct(self):\n",
    "        \"\"\"\n",
    "        Calcul le hash qui déterminera si un element à changé au court du temps\n",
    "        \"\"\"\n",
    "        integrity = ''\n",
    "        if self.title != None:\n",
    "            integrity += self.title\n",
    "        if self.summary != None:\n",
    "            integrity += self.summary\n",
    "        if self.description != None:\n",
    "            integrity += self.description\n",
    "        if self.target_data != None:\n",
    "            integrity += self.target_data\n",
    "        self.integrity = hashlib.sha224(integrity.encode(encoding='UTF-8')).hexdigest()\n",
    "\n",
    "    def save(self):\n",
    "        if self.tool != None :\n",
    "            self.tool.insertion_item(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseTool:\n",
    "\n",
    "    _db = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self._db = self.getConnection()\n",
    "\n",
    "    def getConnection(self, database_name='database'):\n",
    "        return shelve.open(self.database_name, 'c')\n",
    "\n",
    "    def insertion_items(self, _items) :\n",
    "        for i in _items :\n",
    "            self.insertion_item(i)\n",
    "\n",
    "    def insertion_item(self, _item): \n",
    "        if self._db.__contains__(_item.id) == False:\n",
    "            self._db[_item.id] = _item\n",
    "        else : \n",
    "            if _item.integrity != self._db[_item.id].integrity :\n",
    "                self._db[_item.id] = _item\n",
    "\n",
    "    def verification_integrity(self, id_, integrity_):\n",
    "        ret = True\n",
    "        if self._db.__contains__(id_) == False:\n",
    "            ret = False\n",
    "        else : \n",
    "            if integrity_ != self._db[id_].integrity :\n",
    "                ret = False\n",
    "        return ret"
   ]
  },
  {
   "source": [
    "### Crawler"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    nb_crawl_max = 3\n",
    "\n",
    "    def __init__(self, nb_already=0, _save_tool=1):\n",
    "        self.nb_already_done = nb_already\n",
    "        self.save_tool = _save_tool\n",
    "        if self.save_tool == 0 : \n",
    "            self.tool = DatabaseTool()\n",
    "        else : \n",
    "            self.tool = ElasticTool()\n",
    "\n",
    "    def crawl(self, _link, _type_flux=['default']):\n",
    "        if(self.nb_already_done < self.nb_crawl_max):\n",
    "            d = feedparser.parse('%s' % _link)\n",
    "            for post in d.entries:\n",
    "                elem = Item_RSS(post,d.feed, tool=self.tool)\n",
    "                for l in elem.all_links:\n",
    "                    c = Crawler(self.nb_already_done + 1, _save_tool=self.save_tool)\n",
    "                    c.crawl(l['href'], _type_flux=_type_flux)\n",
    "                elem.save()\n",
    "\n",
    "    def crawl_from_file(self, filename):\n",
    "        links = [link.rstrip('\\n').split(' ') for link in fileinput.input(files=(filename))]\n",
    "        for link,*subjects in links:\n",
    "            self.crawl(link, _type_flux=subjects)"
   ]
  },
  {
   "source": [
    "## TP2 - ElasticSearch\n",
    "### ElasticTool"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticTool:\n",
    "\n",
    "    _es = None\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._es = self.getConnection()\n",
    "        self.add_all_index()\n",
    "\n",
    "    def getConnection(self, _host = 'localhost', _port=9200):\n",
    "        if self._es == None :\n",
    "            self._es = Elasticsearch([{'host': _host, 'port': _port}])\n",
    "        return self._es\n",
    "\n",
    "    def affichage_etat(self):\n",
    "        if self._es.ping() :\n",
    "            print('ElasticSearch Tourne')\n",
    "        else :\n",
    "            print('ElasticSearch ne tourne pas')\n",
    "\n",
    "    def add_index(self, name_index):\n",
    "        if self._es.indices.exists(index=name_index):\n",
    "            self._es.indices.create(index=name_index, ignore=400)\n",
    "\n",
    "    def add_all_index(self):\n",
    "        self.add_index(\"item_rss\")\n",
    "        self.add_index(\"title\")\n",
    "        self.add_index(\"summary\")\n",
    "        self.add_index(\"description\")\n",
    "        self.add_index(\"links\")\n",
    "        self.add_index(\"source_post\")\n",
    "        self.add_index(\"lang\")\n",
    "        self.add_index(\"date\")\n",
    "        self.add_index(\"target_data\")\n",
    "        self.add_index('integrity')\n",
    "        self.add_index('type_flux')\n",
    "        self.add_index('type_predit')\n",
    "\n",
    "    def delete_index(self, name_index):\n",
    "        self._es.indices.delete(index=name_index, ignore=[400, 404])\n",
    "\n",
    "    def delete_all_index(self):\n",
    "        self.delete_index(\"item_rss\")\n",
    "        self.delete_index(\"title\")\n",
    "        self.delete_index(\"summary\")\n",
    "        self.delete_index(\"description\")\n",
    "        self.delete_index(\"links\")\n",
    "        self.delete_index(\"source_post\")\n",
    "        self.delete_index(\"lang\")\n",
    "        self.delete_index(\"date\")\n",
    "        self.delete_index(\"target_data\")\n",
    "        self.delete_index('integrity')\n",
    "        self.delete_index('type_flux')\n",
    "        self.delete_index('type_predit')\n",
    "\n",
    "    def insertion_all_items(self, _items):\n",
    "        for i in _items :\n",
    "            self.insertion_item(i)\n",
    "\n",
    "    def insertion_item(self, _item):\n",
    "        if self.verification_integrity(_item.id, _item.integrity) == False :\n",
    "            id_title = self.save_title(_item)\n",
    "            id_summary = self.save_summary(_item)\n",
    "            id_description = self.save_description(_item)\n",
    "            id_all_links = self.save_all_links(_item)\n",
    "            id_source_post = self.save_source_post(_item)\n",
    "            id_lang = self.save_lang(_item)\n",
    "            id_date = self.save_date(_item)\n",
    "            id_target_data = self.save_target_data(_item)\n",
    "            id_integrity = self.save_integrity(_item)\n",
    "            id_type_flux = self.save_type_flux(_item)\n",
    "            id_type_predit = self.save_type_predit(_item)\n",
    "            self.save_item(_item.id, id_title, id_summary, id_description, id_all_links, id_source_post, id_lang, id_date, id_target_data, id_integrity, id_type_flux, id_type_predit)\n",
    "\n",
    "    def save_item(self, _id, _title, _summary, _description, _all_links, _source_post, _lang, _date, _target_data, _integrity, _type_flux, _type_predit ):\n",
    "        content_body = {}\n",
    "        if _title != False :\n",
    "            content_body['id_title'] = _title\n",
    "        if _summary != False :\n",
    "            content_body['id_summary'] = _summary\n",
    "        if _description != False :\n",
    "            content_body['id_description'] = _description\n",
    "        if _all_links != False :\n",
    "            content_body['id_all_links'] = _all_links\n",
    "        if _source_post != False :\n",
    "            content_body['id_source_post'] = _source_post\n",
    "        if _lang != False :\n",
    "            content_body['id_lang'] = _lang\n",
    "        if _date != False :\n",
    "            content_body['id_date'] = _date\n",
    "        if _target_data != False :\n",
    "            content_body['id_target_data'] = _target_data\n",
    "        if _integrity != False :\n",
    "            content_body['id_integrity'] = _integrity\n",
    "        if _type_flux != False :\n",
    "            content_body['type_flux'] = _type_flux\n",
    "        if _type_predit != False :\n",
    "            content_body['type_predit'] = _type_predit\n",
    "        self._es.index(index='item_rss', id=_id, body=content_body)\n",
    "\n",
    "    def save_title(self, _item) : \n",
    "        if(_item.title != None) : \n",
    "            content_body = {\n",
    "                'value' : _item.title,\n",
    "                'tags' : _item.title.split(' '),\n",
    "                'id_item' : _item.id\n",
    "            }\n",
    "            return self._es.index(index=\"title\", body=content_body)['_id']\n",
    "        return False\n",
    "\n",
    "    def save_summary(self, _item):\n",
    "        if _item.summary != None :\n",
    "            content_body = {\n",
    "                'value' : _item.summary,\n",
    "                'tags' : _item.summary.split(' '),\n",
    "                'id_item' : _item.id\n",
    "            }\n",
    "            return self._es.index(index=\"summary\", body=content_body)['_id']\n",
    "        return False\n",
    "\n",
    "    def save_description(self, _item):\n",
    "        if _item.description != None :\n",
    "            content_body = {\n",
    "                'value' : _item.description,\n",
    "                'tags' : _item.description.split(' '),\n",
    "                'id_item' : _item.id\n",
    "            }\n",
    "            return self._es.index(index=\"description\", body=content_body)['_id']\n",
    "        return False\n",
    "\n",
    "    def save_link(self, _link, _id):\n",
    "        content_body = {\n",
    "            'value' : _link,\n",
    "            'id_item' : _id\n",
    "        }\n",
    "        return self._es.index(index=\"links\", body=content_body)['_id']\n",
    "\n",
    "    def save_all_links(self,_item):\n",
    "        id_tab_links = []\n",
    "        if(_item.all_links != None):\n",
    "            for l in _item.all_links:\n",
    "                id_tab_links.append(self.save_link(l, _item.id))\n",
    "        return id_tab_links if len(id_tab_links)>0 else False\n",
    "\n",
    "    def save_source_post(self, _item):\n",
    "        if _item.source_post != None :\n",
    "            content_body = {\n",
    "                'value' : _item.source_post,\n",
    "                'id_item' : _item.id\n",
    "            }\n",
    "            return self._es.index(index=\"source_post\", body=content_body)['_id']\n",
    "        return False\n",
    "\n",
    "    def save_lang(self, _item):\n",
    "        if _item.source_post != None :\n",
    "            content_body = {\n",
    "                'value' : _item.lang,\n",
    "                'id_item' : _item.id\n",
    "            }\n",
    "            return self._es.index(index=\"lang\", body=content_body)['_id']\n",
    "        return False\n",
    "\n",
    "    def save_date(self, _item):\n",
    "        if _item.date != None :\n",
    "            content_body = {\n",
    "                'value' : _item.date,\n",
    "                'id_item' : _item.id\n",
    "            }\n",
    "            return self._es.index(index=\"date\", body=content_body)['_id']\n",
    "        return False\n",
    "\n",
    "    def save_target_data(self, _item):\n",
    "        if _item.target_data != None :\n",
    "            content_body = {\n",
    "                'value' : _item.target_data,\n",
    "                'tags' : _item.target_data.split(' '),\n",
    "                'id_item' : _item.id\n",
    "            }\n",
    "            return self._es.index(index=\"target_data\", body=content_body)['_id']\n",
    "        return False\n",
    "\n",
    "    def save_integrity(self, _item):\n",
    "        if _item.integrity != None :\n",
    "            content_body = {\n",
    "                'value' : _item.integrity,\n",
    "                'id_item' : _item.id\n",
    "            }\n",
    "            return self._es.index(index=\"integrity\", body=content_body)['_id']\n",
    "        return False\n",
    "        \n",
    "    def save_type_flux(self, _item):\n",
    "        if _item.type_flux != None :\n",
    "            type_ = {}\n",
    "            for i in range(len(_item.type_flux)):\n",
    "                type_[str(i)] = _item.type_flux[i]\n",
    "            content_body = {\n",
    "                'value' : type_,\n",
    "                'id_item' : _item.id\n",
    "            }\n",
    "            return self._es.index(index=\"type_flux\", body=content_body)['_id']\n",
    "        return False\n",
    "\n",
    "    def save_type_predit(self, _item):\n",
    "        if _item.type_flux != None :\n",
    "            content_body = {\n",
    "                'value' : _item.type_predit,\n",
    "                'id_item' : _item.id\n",
    "            }\n",
    "            return self._es.index(index=\"type_predit\", body=content_body)['_id']\n",
    "        return False\n",
    "\n",
    "    def search_by_tags(self, index_name, tags, size_result=999):\n",
    "        \n",
    "        str_query = ''\n",
    "        maxi = len(tags)\n",
    "        for i in range(maxi):\n",
    "            str_query += 'tags:' + '*'+tags[i]+'*'\n",
    "            if i < maxi-1:\n",
    "                str_query += ' AND '\n",
    "                \n",
    "        query_body ={\n",
    "            \"query\": {\n",
    "                \"query_string\": {\n",
    "                    \"query\" : str_query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        return self._es.search(index=index_name, body=query_body, size=size_result)['hits']['hits']\n",
    "\n",
    "    def verification_integrity(self, id_, integrity_):\n",
    "        res = self._es.get(index=\"integrity\", id=id_, ignore=[400,404])\n",
    "        return False\n",
    "\n",
    "    def get_all_from_index(self, index_name, size_result=999):\n",
    "        query_body = {\n",
    "            'size' : 100,\n",
    "            'query': {\n",
    "                'match_all' : {}\n",
    "            }\n",
    "        }\n",
    "        return self._es.search(index=index_name, body=query_body, size=size_result)['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticToolv0 :\n",
    "\n",
    "    _es = None\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._es = self.getConnection()\n",
    "        self.add_index()\n",
    "\n",
    "    def getConnection(self, _host = 'localhost', _port=9200):\n",
    "        if self._es == None :\n",
    "            self._es = Elasticsearch([{'host': _host, 'port': _port}])\n",
    "        return self._es\n",
    "\n",
    "    def affichage_etat(self):\n",
    "        if self._es.ping() :\n",
    "            print('ElasticSearch Tourne')\n",
    "        else :\n",
    "            print('ElasticSearch ne tourne pas')\n",
    "\n",
    "    def add_index(self):\n",
    "        if self._es.indices.exists(index='item'):\n",
    "            self._es.indices.create(index='item', ignore=400)\n",
    "\n",
    "    def delete_index(self):\n",
    "        self._es.indices.delete(index='item', ignore=[400, 404])\n",
    "\n",
    "    def insertion_item(self, _item) :\n",
    "        content_body = {\n",
    "            'title' : _item.title,\n",
    "            'summary' : _item.summary,\n",
    "            'description' : _item.description,\n",
    "            'all_links' : _item.all_links,\n",
    "            'source_post' : _item.source_post,\n",
    "            'source_feed' : _item.source_feed,\n",
    "            'lang' : _item.lang,\n",
    "            'date' : _item.date,\n",
    "            'target_data' :  _item.target_data,\n",
    "            'type_flux' : _item.type_flux,\n",
    "            'type_predit' : _item.type_predit,\n",
    "            'tags' : np.append(_item.description.split(' '), _item.summary.split(' '),_item.title.split(' '), _item.target_data.split(' '))\n",
    "        }\n",
    "        return self._es.index(index='item', id=_item.id, body=content_body)\n",
    "\n",
    "    def search_by_tags(self, tags, size_result=999):\n",
    "        \n",
    "        str_query = ''\n",
    "        maxi = len(tags)\n",
    "        for i in range(maxi):\n",
    "            str_query += 'tags:' + '*'+tags[i]+'*'\n",
    "            if i < maxi-1:\n",
    "                str_query += ' AND '\n",
    "                \n",
    "        query_body ={\n",
    "            \"query\": {\n",
    "                \"query_string\": {\n",
    "                    \"query\" : str_query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        return self._es.search(index='item', body=query_body, size=size_result)['hits']['hits']"
   ]
  },
  {
   "source": [
    "## TP 3 - Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Execution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_tool = ElasticTool()\n",
    "elastic_tool.affichage_etat()"
   ]
  },
  {
   "source": [
    "## Lancement"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cr = Crawler()\n",
    "cr.crawl(\"https://www.lefigaro.fr/rss/figaro_economie.xml\",_type_flux=['ECONOMIE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = Crawler()\n",
    "# ================= Necessite un fichier flux_rss.txt fomat sur une ligne:  lien type\n",
    "cr.crawl_from_file('flux_rss.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_tool = ElasticTool()\n",
    "print(elastic_tool.search_by_tags('description',['essentiel']))\n",
    "elastic_tool.search_by_tags('description', ['commerces', 'rouvrir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_tool = ElasticTool()\n",
    "elastic_tool.delete_all_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 3 were given",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6299040da750>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvaleur\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtest2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "class MyClass:\n",
    "    def __init__(self, valeur, val):\n",
    "        self.val1 = valeur\n",
    "        self.val2 = val\n",
    "    def __init__(self):\n",
    "        self.valeur = \"test\"\n",
    "test = MyClass()\n",
    "print(test.valeur)\n",
    "test2 = MyClass(12,14)\n",
    "print(test2.val1, test2.val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}