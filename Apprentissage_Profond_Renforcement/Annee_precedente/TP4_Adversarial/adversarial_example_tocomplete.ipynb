{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non targeted and targeted Adversarial Examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import network.network as Network\n",
    "import network.mnist_loader as mnist_loader\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load up the MNIST data. The network we unpickle has one hidden layer of 30 units, 784 input units and 10 output units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('network/trained_network.pkl', 'rb') as f:\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'latin1'\n",
    "    net = u.load()\n",
    "    \n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run predict with any number between 0 and 9999. The output of the network is a one-hot vector indicating the network's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(n):\n",
    "    # Get the data from the test set\n",
    "    l = [data for data in test_data]\n",
    "    x = l[n][0]\n",
    "\n",
    "    # Print the prediction of the network\n",
    "    print('Network output: \\n' + str(np.round(net.feedforward(x), 2)) + '\\n')\n",
    "    print('Network prediction: ' + str(np.argmax(net.feedforward(x))) + '\\n')\n",
    "    print('Actual image: ')\n",
    "    \n",
    "    # Draw the image\n",
    "    plt.imshow(x.reshape((28,28)), cmap='Greys')\n",
    "\n",
    "#to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to generate an adversarial example, we want the network to think our image is an other number with a 1 at the ieme place of the one-hot vector. Our goal is to find an $ \\vec x $ such that $ C $ our cost function is minimize. We initialize a random vector $ \\vec x $ and we apply gradient descent on $ C $. We change $ \\vec x $ at each step because our goal is now to change only the input of the network, we keep the weights and biases constant. The cost function $ C $ is define :\n",
    "$$ C = \\frac{1}{2} \\|\\vec y_{goal} - \\hat y(\\vec x)\\|^2_2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement the `sigmoid` and its element-wise derivative `dsigmoid` functions:\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dsigmoid(x) = sigmoid(x) \\cdot (1 - sigmoid(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    #to complete\n",
    "                                                                                                                                                                                \n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    #to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, a function to gradient the derivative of the cost function, $ \\nabla_x C $ with respect to the input $ \\vec x $, with a goal label of $ \\vec y_{goal} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_derivative(net, x, y):\n",
    "    \"\"\" Calculate derivatives wrt the inputs\"\"\"\n",
    "    nabla_b = [np.zeros(b.shape) for b in net.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in net.weights]\n",
    "    \n",
    "    # feedforward\n",
    "    activation = x\n",
    "    activations = [x] # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer\n",
    "    for b, w in zip(net.biases, net.weights):\n",
    "        z = np.dot(w, activation)+b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "        \n",
    "    # backward pass\n",
    "    delta = net.cost_derivative(activations[-1], y) * \\\n",
    "        sigmoid_prime(zs[-1])\n",
    "    nabla_b[-1] = delta\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "    for l in range(2, net.num_layers):\n",
    "        z = zs[-l]\n",
    "        sp = sigmoid_prime(z)\n",
    "        delta = np.dot(net.weights[-l+1].transpose(), delta) * sp\n",
    "        nabla_b[-l] = delta\n",
    "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        \n",
    "    # Return derivatives WRT to input\n",
    "    return net.weights[0].T.dot(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual function that generates adversarial examples and a wrapper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial(net, n, steps, eta):\n",
    "    \"\"\"\n",
    "    net : network object\n",
    "        neural network instance to use\n",
    "    n : integer\n",
    "        our goal label (just an int, the function transforms it into a one-hot vector)\n",
    "    steps : integer\n",
    "        number of steps for gradient descent\n",
    "    eta : float\n",
    "        step size for gradient descent\n",
    "    \"\"\"\n",
    "    # Set the goal output (output vector of the neural network)  \n",
    "    #set the n case to 1\n",
    "    # Create a random image to initialize gradient descent with using np.random.normal\n",
    "   \n",
    "    # Gradient descent on the input\n",
    "    #We calculate the derivative for each step and update the gradient\n",
    "    #we return the gradient value\n",
    "\n",
    "# Wrapper function\n",
    "def generate(n):\n",
    "    \"\"\"\n",
    "    n : integer\n",
    "        goal label (not a one hot vector)\n",
    "    \"\"\"\n",
    "    a = adversarial(net, n, 1000, 1)\n",
    "    x = np.round(net.feedforward(a), 2)\n",
    "    \n",
    "    print('Network Output: \\n' + str(x) + '\\n')\n",
    "    \n",
    "    print('Network Prediction: ' + str(np.argmax(x)) + '\\n')\n",
    "    \n",
    "    print('Adversarial Example: ')\n",
    "    plt.imshow(a.reshape(28,28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate an adversarial examples with generate. You can call generate with a number between 0 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate an image that look like the same to the image, but not for the neural network. We have now a new cost function $ C $ :\n",
    "$$ C = \\|\\vec y_{goal} - y_{hat}(\\vec x)\\|^2_2 + \\lambda \\|\\vec x - \\vec x_{target}\\|^2_2 $$\n",
    "\n",
    "We now want to minimize $ C $ and the distance between $ \\vec x $ and $ \\vec x_{target} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sneaky_adversarial(net, n, x_target, steps, eta, lam=.05):\n",
    "    \"\"\"\n",
    "    net : network object\n",
    "        neural network instance to use\n",
    "    n : integer\n",
    "        our goal label (just an int, the function transforms it into a one-hot vector)\n",
    "    x_target : numpy vector\n",
    "        our goal image for the adversarial example\n",
    "    steps : integer\n",
    "        number of steps for gradient descent\n",
    "    eta : float\n",
    "        step size for gradient descent\n",
    "    lam : float\n",
    "        lambda, our regularization parameter. Default is .05\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the goal output (output vector of the neural network)      \n",
    "    #set the n case to 1\n",
    "    # Create a random image to initialize gradient descent with using np.random.normal\n",
    "    \n",
    "    # Gradient descent on the input\n",
    "    #We calculate the derivative for each step and update the gradient (this time with an added penalty\n",
    "    #to the cost function)\n",
    "    #we return the gradient value\n",
    "\n",
    "# Wrapper function\n",
    "def sneaky_generate(n, m):\n",
    "    \"\"\"\n",
    "    n: int 0-9, the target number to match\n",
    "    m: index of example image to use (from the test set)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find random instance of m in test set\n",
    "    idx = np.random.randint(0,8000)\n",
    "    l = [data for data in test_data]\n",
    "    while l[idx][1] != m:\n",
    "        idx += 1\n",
    "    \n",
    "    # Hardcode the parameters for the wrapper function\n",
    "    a = sneaky_adversarial(net, n, l[idx][0], 100, 1)\n",
    "    x = np.round(net.feedforward(a), 2)\n",
    "    \n",
    "    print('\\nWhat we want our adversarial example to look like: ')\n",
    "    plt.imshow(l[idx][0].reshape((28,28)), cmap='Greys')\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    print('Adversarial Example: ')\n",
    "    \n",
    "    plt.imshow(a.reshape(28,28), cmap='Greys')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Network Prediction: ' + str(np.argmax(x)) + '\\n')\n",
    "    \n",
    "    print('Network Output: \\n' + str(x) + '\\n')\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can generate your own adversarial example, try with 0, 2, 3, 5, 6, or 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sneaky_generate(target label, target digit)\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "#Generate your own example\n",
    "\n",
    "#to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate adversarial examples, but now let's defend against them, one simple method is to us binary thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_thresholding(n, m):\n",
    "    \"\"\"\n",
    "    n: int 0-9, the target number to match\n",
    "    m: index of example image to use (from the test set)\n",
    "    \"\"\"\n",
    "    \n",
    "    x = sneaky_generate(n, m)\n",
    "\n",
    "    #remove noise from the image with a thresholding\n",
    "    #to complete \n",
    "    \n",
    "    \n",
    "    print(\"With binary thresholding: \")\n",
    "    \n",
    "    plt.imshow(x.reshape(28,28), cmap=\"Greys\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Prediction with binary thresholding: \" + str(np.argmax(np.round(net.feedforward(x)))) + '\\n')\n",
    "    \n",
    "    print(\"Network output: \")\n",
    "    print(np.round(net.feedforward(x), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_thresholding(target digit, actual digit)\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "#to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple method is to generate adversarial examples and give them to the neural network, the goal is to make the neural network learn to ignore adversarial examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(n, data, steps):\n",
    "    \"\"\"\n",
    "    n : integer\n",
    "        number of adversarial examples to generate\n",
    "    data : list of tuples\n",
    "        data set to generate adversarial examples using\n",
    "    \"\"\"\n",
    "    # Our augmented training set:\n",
    "    augmented = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Progress \"bar\"\n",
    "        if i % 500 == 0:\n",
    "            print(\"Generated digits: \" + str(i))\n",
    "            \n",
    "        # Randomly choose a digit that the example will look like\n",
    "        rnd_actual_digit = np.random.randint(10)\n",
    "        \n",
    "        # Find random instance of rnd_actual_digit in the training set\n",
    "        rnd_actual_idx = np.random.randint(len(data))\n",
    "        while np.argmax(data[rnd_actual_idx][1]) != rnd_actual_digit:\n",
    "            rnd_actual_idx = np.random.randint(len(data))\n",
    "        x_target = data[rnd_actual_idx][0]\n",
    "        \n",
    "        # Choose value for adversarial attack\n",
    "        rnd_fake_digit = np.random.randint(10)\n",
    "        \n",
    "        # Generate adversarial example\n",
    "        x_adversarial = sneaky_adversarial(net, rnd_fake_digit, x_target, steps, 1)\n",
    "        \n",
    "        # Add new data\n",
    "        y_actual = data[rnd_actual_idx][1]\n",
    "        \n",
    "        augmented.append((x_adversarial, y_actual))\n",
    "        \n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take quite a while (~3 min for 10000, ~15 for 50000)\n",
    "# Try 10000 examples first if you don't want to wait\n",
    "augmented = augment_data(10000, list(training_data), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_augmented(i, augmented):\n",
    "    # Show image\n",
    "    print('Image: \\n')\n",
    "    plt.imshow(augmented[i][0].reshape(28,28), cmap='Greys')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show original network prediction\n",
    "    print('Original network prediction: \\n')\n",
    "    print(np.round(net.feedforward(augmented[i][0]), 2))\n",
    "    \n",
    "    # Show label\n",
    "    print('\\nLabel: \\n')\n",
    "    print(augmented[i][1])\n",
    "\n",
    "# check i^th adversarial image\n",
    "check_augmented(239, augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a new neural network using the library Network and train it on our augmented dataset and the original training set, using the original test set to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new network with network\n",
    "# to complete\n",
    "\n",
    "#reload data\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "# Train on augmented + original training set with SGD\n",
    "#to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a test set of adversarial examples by using the following function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For some reason the training data has the format: list of tuples\n",
    "# tuple[0] is np array of image\n",
    "# tuple[1] is one hot np array of label\n",
    "# test data is also list of tuples\n",
    "# tuple[0] is np array of image\n",
    "# tuple[1] is integer of label\n",
    "# Just fixing this:\n",
    "normal_test_data = []\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "l = [data for data in test_data]\n",
    "for i in range(len(l)):\n",
    "    ground_truth = l[i][1]\n",
    "    one_hot = np.zeros(10)\n",
    "    one_hot[ground_truth] = 1\n",
    "    one_hot = np.expand_dims(one_hot, axis=1)\n",
    "    normal_test_data.append((l[i][0], one_hot))\n",
    "    \n",
    "\n",
    "# Using normal_test_data because of weird way data is packaged\n",
    "\n",
    "#call augment_data\n",
    "#to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's checkout the accuracy of our newly trained network on adversarial examples from the new adversarial test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(net, test_data):\n",
    "    \"\"\"\n",
    "    net : network object\n",
    "    test_data: list\n",
    "        list of 2-tuples of two arrays, one image and one label (one-hot)\n",
    "    \"\"\"\n",
    "    #to complete\n",
    "    \n",
    "#print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, a function that compares the original network to the new network on adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_test(net, net2, test_data, n):\n",
    "    print('Original network prediction: ' + str(np.argmax(np.round(net.feedforward(test_data[n][0])))) + '\\n')\n",
    "    print('New network prediction: ' + str(np.argmax(np.round(net2.feedforward(test_data[n][0])))) + '\\n')\n",
    "    print('Image: \\n')\n",
    "    plt.imshow(test_data[n][0].reshape(28,28), cmap='Greys')\n",
    "\n",
    "#call sample test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
