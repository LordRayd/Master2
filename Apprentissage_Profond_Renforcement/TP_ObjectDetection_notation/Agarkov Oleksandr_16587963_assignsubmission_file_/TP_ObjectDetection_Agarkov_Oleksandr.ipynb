{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP_ObjectDetection_solution_toPut.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a9atjgk4cZt"
      },
      "source": [
        "# <center>Mask R-CNN - Agarkov Oleksandr</center>\n",
        "Le but de ce TP est d'apprendre le fonctionnement des algorithmes de reconnaissance d'objet.\n",
        "Pour cela dans ce TP vous verrez :\n",
        "\n",
        "- Comment fonctionne les masques\n",
        "- Comment fonctionne les Bounding Box\n",
        "- Affichage des résultats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSVqhihf5qLz",
        "outputId": "a2ae1900-b549-4c3c-dc6a-146928c83016"
      },
      "source": [
        "!pip install --upgrade opencv-python\n",
        "!pip install gdown"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: opencv-python in /usr/local/lib/python3.6/dist-packages (4.5.1.48)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.19.4)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMoAKWTD4VVl"
      },
      "source": [
        "import gdown\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from cv2.dnn import readNetFromTensorflow, blobFromImage\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njqVxRVv5lx_",
        "outputId": "a8332ac3-02ad-4e6e-816d-1d16a5d643fc"
      },
      "source": [
        "gdown.download(\"https://drive.google.com/uc?id=1StvgNbOx-22h6A57H5NqG2GP8qbWzrs-\", \"LIB.tar.gz\", quiet = False)\n",
        "!tar zxvf LIB.tar.gz\n",
        "!rm LIB.tar.gz"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1StvgNbOx-22h6A57H5NqG2GP8qbWzrs-\n",
            "To: /content/LIB.tar.gz\n",
            "60.6MB [00:00, 232MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "./._colors.txt\n",
            "colors.txt\n",
            "./._frozen_inference_graph.pb\n",
            "frozen_inference_graph.pb\n",
            "./._mask_rcnn_inception_v2_coco_2018_01_28.pbtxt\n",
            "mask_rcnn_inception_v2_coco_2018_01_28.pbtxt\n",
            "./._mscoco_labels.names\n",
            "mscoco_labels.names\n",
            "./._people_walking.mp4\n",
            "people_walking.mp4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYDTvwp2GDO2"
      },
      "source": [
        "***Initialisation des paramètres :***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCMPf3Or6HoG"
      },
      "source": [
        "# Initialize the parameters\n",
        "confThreshold = 0.5  #Confidence threshold\n",
        "maskThreshold = 0.3  # Mask threshold"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhWDnyGODeKm"
      },
      "source": [
        "***Récupération des labels,du graphe et des poids du modèle :***\n",
        "\n",
        "Objectif :\n",
        "  - Récupération des labels à partir du fichier mscoco_labels.names\n",
        "  - Récupération des couleurs pour les bounding boxes à partir du fichier colors.txt\n",
        "  - Setup du modèle\n",
        "    - Le graph du réseau se trouve dans mask_rcnn_inception_v2_coco_2018_01_28.pbtxt\n",
        "    - Les poifs se trouvent dans frozen_inference_graph.pb\n",
        "\n",
        "Pour vous aidez dans le chargement du réseau, regarder les méthode suivantes :\n",
        "  - cv.dnn.readNetFromTensorFlow\n",
        "  - setPreferableBackend\n",
        "  - setPreferableTarget\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NadaIgOD98aw"
      },
      "source": [
        "# Load names of classes\n",
        "classesFile = \"mscoco_labels.names\";\n",
        "classes = None\n",
        "with open(classesFile, 'rt') as f:\n",
        "   classes = f.read().rstrip('\\n').split('\\n')\n",
        "\n",
        "# Load the colors\n",
        "colorsFile = \"colors.txt\"\n",
        "with open(colorsFile, 'rt') as f:\n",
        "    colorsStr = f.read().rstrip('\\n').split('\\n')\n",
        "colors = []\n",
        "for i in range(len(colorsStr)):\n",
        "    rgb = colorsStr[i].split(' ')\n",
        "    color = np.array([float(rgb[0]), float(rgb[1]), float(rgb[2])])\n",
        "    colors.append(color)\n",
        "\n",
        "# Give the textGraph and weight files for the model\n",
        "textGraph = \"./mask_rcnn_inception_v2_coco_2018_01_28.pbtxt\";\n",
        "modelWeights = \"./frozen_inference_graph.pb\";\n",
        "\n",
        "# Load the network\n",
        "net = cv.dnn.readNetFromTensorflow(modelWeights, textGraph)\n",
        "net.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)\n",
        "net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzFQMw9fHq1i"
      },
      "source": [
        "***Extraction des bounding boxes et des masques pour les objets détectés :***\n",
        "\n",
        "Objectif :\n",
        "  - Extraire les données des bounding boxes\n",
        "  - Extraire les masques\n",
        "  - Dessiner les boxes avec toutes les informations nécessaires\n",
        "\n",
        "Les données boxes et masks sont sous les formes :\n",
        "  - boxes : [0,0,nbId]\n",
        "    - box : (_, classeID, score, left, top, right, bottom)\n",
        "  - masks : (Number of detected object, Number of classes, (Segmentation shape))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O5hV3yi_ALT"
      },
      "source": [
        "# For each frame, extract the bounding box and mask for each detected object\n",
        "def postprocess(frame, boxes, masks):\n",
        "    numClasses = masks.shape[1]\n",
        "    numDetections = boxes.shape[2]\n",
        "     \n",
        "    frameH = frame.shape[0]\n",
        "    frameW = frame.shape[1]\n",
        "     \n",
        "    # Traité toute les detections\n",
        "    for i in range(numDetections):\n",
        "        box = boxes[0, 0, i]\n",
        "        mask = masks[i]\n",
        "        score = box[2]\n",
        "        if score > confThreshold:\n",
        "            classId = int(box[1])\n",
        "            \n",
        "            # Extract the bounding box\n",
        "            left = int(frameW * box[3])\n",
        "            top = int(frameH * box[4])\n",
        "            right = int(frameW * box[5])\n",
        "            bottom = int(frameH * box[6])\n",
        "            \n",
        "            left = max(0, min(left, frameW - 1))\n",
        "            top = max(0, min(top, frameH - 1))\n",
        "            right = max(0, min(right, frameW - 1))\n",
        "            bottom = max(0, min(bottom, frameH - 1))\n",
        "            \n",
        "            # Extract the mask for the object\n",
        "            classMask = mask[classId]\n",
        "            # Draw bounding box, colorize and show the mask on the image\n",
        "            drawBox(frame, classId, score, left, top, right, bottom, classMask)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV2bO_SR_Zkf"
      },
      "source": [
        "# Draw the predicted bounding box, colorize and show the mask on the image\n",
        "def drawBox(frame, classId, conf, left, top, right, bottom, classMask):\n",
        "    # Draw a bounding box.\n",
        "    cv.rectangle(frame, (left, top), (right, bottom), (255, 178, 50), 3)\n",
        "     \n",
        "    # Print a label of class.\n",
        "    label = '%.2f' % conf\n",
        "\n",
        "    if classes:\n",
        "        assert(classId < len(classes))\n",
        "        label = '%s:%s' % (classes[classId], label)\n",
        "     \n",
        "    # Display the label at the top of the bounding box\n",
        "    labelSize, baseLine = cv.getTextSize(label, cv.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
        "    top = max(top, labelSize[1])\n",
        "    cv.rectangle(frame, (left, top - round(1.5*labelSize[1])), (left + round(1.5*labelSize[0]), top + baseLine), (255, 255, 255), cv.FILLED)\n",
        "    cv.putText(frame, label, (left, top), cv.FONT_HERSHEY_SIMPLEX, 0.75, (0,0,0), 1)\n",
        " \n",
        "    # Resize the mask, threshold, color and apply it on the image\n",
        "    classMask = cv.resize(classMask, (right - left + 1, bottom - top + 1))\n",
        "    mask = (classMask > maskThreshold)\n",
        "    roi = frame[top:bottom+1, left:right+1][mask]\n",
        " \n",
        "    color = colors[classId%len(colors)]\n",
        "    # Comment the above line and uncomment the two lines below to generate different instance colors\n",
        "    #colorIndex = random.randint(0, len(colors)-1)\n",
        "    #color = colors[colorIndex]\n",
        " \n",
        "    frame[top:bottom+1, left:right+1][mask] = ([0.3*color[0], 0.3*color[1], 0.3*color[2]] + 0.7 * roi).astype(np.uint8)\n",
        " \n",
        "    # Draw the contours on the image\n",
        "    mask = mask.astype(np.uint8)\n",
        "    contours, hierarchy = cv.findContours(mask,cv.RETR_TREE,cv.CHAIN_APPROX_SIMPLE)\n",
        "    cv.drawContours(frame[top:bottom+1, left:right+1], contours, -1, color, 3, cv.LINE_8, hierarchy, 100)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRLPdJu0N8vU"
      },
      "source": [
        "***Affichage des résultats :***\n",
        "\n",
        "Selection du fichier d'input\n",
        "\n",
        "Selection du fichier d'output\n",
        "\n",
        "Regarder la fonction blobFromImage pour obtenir les blob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qggT3hewPP-M"
      },
      "source": [
        "inputFile = \"/people_walking.mp4\"\n",
        "outputFile = \"/people_walking_out.mp4\"\n",
        "cap = cv.VideoCapture(inputFile)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWZwo-r5_nPU",
        "outputId": "ab969467-c15f-413b-ce53-30059fff6035"
      },
      "source": [
        "while cv.waitKey(1) < 0:\n",
        "    # Get frame from the video\n",
        "    hasFrame, frame = cap.read()\n",
        "    \n",
        "    # Stop the program if reached end of video\n",
        "    if not hasFrame:\n",
        "        print(\"Salut salut..Ça va?\")\n",
        "        print(\"Done processing !!!\")\n",
        "        print(\"Output file is stored as \", outputFile)\n",
        "        cv.waitKey(3000)\n",
        "        break\n",
        "\n",
        "    # Create a 4D blob from a frame.\n",
        "    blob = cv.dnn.blobFromImage(frame, swapRB=True, crop=False)\n",
        "\n",
        "    # Set the input to the network\n",
        "    net.setInput(blob)\n",
        "\n",
        "    # Run the forward pass to get output from the output layers\n",
        "    boxes, masks = net.forward(['detection_out_final', 'detection_masks'])\n",
        "\n",
        "    # Extract the bounding box and mask for each of the detected objects\n",
        "    postprocess(boxes, masks)\n",
        "\n",
        "    # Put efficiency information.\n",
        "    t, _ = net.getPerfProfile()\n",
        "    label = 'Mask-RCNN : Inference time: %.2f ms' % (t * 1000.0 / cv.getTickFrequency())\n",
        "    cv.putText(frame, label, (0, 15), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0))\n",
        " \n",
        "    cv.VideoWriter(outputFile,cv.VideoWriter_fourcc('M','J','P','G'), 10, (frame.shape[1],frame.shape[0]))\n",
        "\n",
        "    # Ecriture du fichier\n",
        "    # Write the frame with the detection boxes\n",
        "    # if (args.image):\n",
        "    #     cv.imwrite(outputFile, frame.astype(np.uint8));\n",
        "    # else:\n",
        "    #     vid_writer.write(frame.astype(np.uint8))    \n",
        "    # cv2_imshow(frame)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Salut salut..Ça va?\n",
            "Done processing !!!\n",
            "Output file is stored as  /people_walking_out.mp4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QguoLrHSAabV"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}